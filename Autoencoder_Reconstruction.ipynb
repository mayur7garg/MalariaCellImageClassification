{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit"
  },
  "interpreter": {
   "hash": "dcbe076a40d8142e585077643c26fc4a9c0eed423ce3f041c8a5b2e5c8137bb1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Autoencoder for uninfected malaria cell images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, MaxPooling2D, UpSampling2D, Dropout, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "source": [
    "### Set logging to Error only"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "source": [
    "### Check for GPU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "physical_devices"
   ]
  },
  {
   "source": [
    "## Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 7\n",
    "BASE_PATH = r\"..\\..\\Datasets\\Malaria Cell Images\\Uninfected\"\n",
    "IMAGE_SIZE = (128, 128)\n",
    "VAL_SIZE = 0.05\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = 1_000\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.002\n",
    "PLOTS_DPI = 200\n",
    "MODEL_NAME = 'Autoencoder_Reconstruction'\n",
    "PLOTS_DIR = os.path.join('plots', MODEL_NAME)\n",
    "TB_LOGS = \"tensorboard_logs/Autoencoder_Reconstruction/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "source": [
    "## Data Loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = Dataset.list_files(os.path.join(BASE_PATH, '*.png'), seed = RANDOM_STATE)\n",
    "image_count = image_names.cardinality().numpy()\n",
    "print(f\"\\nTotal number of image files: {image_count}\")"
   ]
  },
  {
   "source": [
    "### Image data loading with augmentations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_augmented_images(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(img, channels = 3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "\n",
    "    img_rotneg90 = tf.image.rot90(img, k = -1)\n",
    "    img_rotpos90 = tf.image.rot90(img, k = 1)\n",
    "\n",
    "    return img_rotneg90/255.0, img/255.0, img_rotpos90/255.0\n",
    "\n",
    "image_data = image_names.map(load_augmented_images, num_parallel_calls = AUTOTUNE)\n",
    "image_data = image_data.flat_map(lambda rotneg90, original, rotpos90: Dataset.from_tensor_slices([rotneg90, original, rotpos90]))\n"
   ]
  },
  {
   "source": [
    "### Augmented data visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_suffixes = ['Rotated -90', 'Original', 'Rotated +90']\n",
    "\n",
    "plt.subplots(nrows = 6, ncols = 6, figsize = (18, 18))\n",
    "plt.suptitle('Uninfected cell images with Image Augmentation', fontsize = 24)\n",
    "plt.tight_layout(rect = [0, 0, 1, 0.97], h_pad = 2)\n",
    "\n",
    "for i, img in enumerate(image_data.take(36)):\n",
    "    plt.subplot(6, 6, i + 1)\n",
    "    plt.axis(False)\n",
    "    plt.grid(False)\n",
    "    plt.title(f\"Img {i + 1} - {title_suffixes[i % 3]}\")\n",
    "    plt.imshow(img.numpy())"
   ]
  },
  {
   "source": [
    "### Data splitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance(ds):\n",
    "  ds = ds.cache()\n",
    "  ds = ds.shuffle(buffer_size = SHUFFLE_BUFFER_SIZE)\n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "  ds = ds.prefetch(buffer_size = AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "def create_autoencoder_dataset(img):\n",
    "    return img, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image_count = int(image_count * VAL_SIZE * 3)\n",
    "\n",
    "train_images = image_data.skip(val_image_count)\n",
    "val_images = image_data.take(val_image_count)\n",
    "\n",
    "train_ds = configure_for_performance(train_images.map(create_autoencoder_dataset))\n",
    "val_ds = configure_for_performance(val_images.map(create_autoencoder_dataset))"
   ]
  },
  {
   "source": [
    "## Model Creation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Input Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayer = Input(shape = (*IMAGE_SIZE, 3), name = 'Input')"
   ]
  },
  {
   "source": [
    "### Encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_conv = DepthwiseConv2D((4, 4), activation = LeakyReLU(), padding = 'same', depth_multiplier = 2, name = \"Depth_Conv\")(inputLayer)\n",
    "depth_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1 = Conv2D(8, (4, 4), activation = LeakyReLU(), padding = 'same', name = \"Enc_Conv_1\")(depth_conv)\n",
    "pool_1 = MaxPooling2D((4, 4), padding = 'same', name = \"Enc_MaxPool_1\")(conv_1)\n",
    "pool_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2 = Conv2D(16, (4, 4), activation = LeakyReLU(), padding = 'same', name = \"Enc_Conv_2\")(pool_1)\n",
    "pool_2 = MaxPooling2D((4, 4), padding = 'same', name = \"Enc_MaxPool_2\")(conv_2)\n",
    "pool_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3 = Conv2D(32, (3, 3), activation = LeakyReLU(), padding = 'same', name = \"Enc_Conv_3\")(pool_2)\n",
    "pool_3 = MaxPooling2D((2, 2), padding = 'same', name = \"Enc_MaxPool_3\")(conv_3)\n",
    "pool_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_4 = Conv2D(64, (3, 3), activation = LeakyReLU(), padding = 'same', name = \"Enc_Conv_4\")(pool_3)\n",
    "pool_4 = MaxPooling2D((2, 2), padding = 'same', name = \"Enc_MaxPool_4\")(conv_4)\n",
    "pool_4.shape"
   ]
  },
  {
   "source": [
    "### Decoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_5 = Conv2D(64, (3, 3), activation = LeakyReLU(), padding = 'same', name = \"Dec_Conv_1\")(pool_4)\n",
    "up_1 = UpSampling2D((2, 2), name = \"Dec_Upsampling_1\")(conv_5)\n",
    "up_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_6 = Conv2D(32, (3, 3), activation = LeakyReLU(), padding = 'same', name = \"Dec_Conv_2\")(up_1)\n",
    "up_2 = UpSampling2D((2, 2), name = \"Dec_Upsampling_2\")(conv_6)\n",
    "up_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_7 = Conv2D(16, (4, 4), activation = LeakyReLU(), padding = 'same', name = \"Dec_Conv_3\")(up_2)\n",
    "up_3 = UpSampling2D((4, 4), name = \"Dec_Upsampling_3\")(conv_7)\n",
    "up_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_8 = Conv2D(8, (4, 4), activation = LeakyReLU(), padding = 'same', name = \"Dec_Conv_4\")(up_3)\n",
    "up_4 = UpSampling2D((4, 4), name = \"Dec_Upsampling_4\")(conv_8)\n",
    "up_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = Dropout(0.1, name = \"Dropout\")(up_4)\n",
    "dropout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputLayer = Conv2D(3, (1, 1), activation = 'sigmoid', padding = 'same', name = 'Reconstruction_Output')(dropout)\n",
    "outputLayer.shape"
   ]
  },
  {
   "source": [
    "### Model compilation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(inputs = inputLayer, outputs = outputLayer, name = MODEL_NAME)\n",
    "autoencoder.compile(optimizer = Adam(LEARNING_RATE), loss = 'binary_crossentropy')"
   ]
  },
  {
   "source": [
    "### Model Summary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(autoencoder, to_file = os.path.join(PLOTS_DIR, 'model.jpg'), show_shapes = True, dpi = PLOTS_DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "source": [
    "## Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Callbacks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True)\n",
    "tensorboard = TensorBoard(log_dir = TB_LOGS)\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 4, verbose = 1, cooldown = 1)"
   ]
  },
  {
   "source": [
    "### Training history"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    train_ds, \n",
    "    epochs = EPOCHS, \n",
    "    verbose = 1, \n",
    "    validation_data = val_ds,\n",
    "    callbacks = [early_stop, tensorboard, reduce_lr]\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Model Loss over Epochs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = history.epoch\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "plt.plot(epochs_range, loss, label = 'Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label = 'Validation Loss')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig(os.path.join(PLOTS_DIR, 'acc_and_loss.jpg'), dpi = PLOTS_DPI, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Prediction Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(nrows = 6, ncols = 6, figsize = (18, 18))\n",
    "\n",
    "plt.suptitle('Autoencoder predictions on uninfected cells', fontsize = 24)\n",
    "plt.tight_layout(rect = [0, 0, 1, 0.97], h_pad = 2)\n",
    "\n",
    "for i in val_ds.take(1):\n",
    "    val_data = i[0][:12].numpy()\n",
    "\n",
    "val_data = np.array(val_data)\n",
    "pred = autoencoder.predict(val_data)\n",
    "pred_error = val_data - pred\n",
    "pred_error_min = pred_error.min(axis = (1, 2, 3)).reshape(12, 1, 1, 1)\n",
    "pred_error_max = pred_error.max(axis = (1, 2, 3)).reshape(12, 1, 1, 1)\n",
    "norm_error = (pred_error - pred_error_min)/(pred_error_max - pred_error_min)\n",
    "\n",
    "for i in range(len(val_data)):\n",
    "    plt.subplot(6, 6, (3 * i) + 1)\n",
    "    plt.axis(False)\n",
    "    plt.grid(False)\n",
    "    plt.title(f\"Original - {i + 1}\")\n",
    "    plt.imshow(val_data[i])\n",
    "\n",
    "    plt.subplot(6, 6, (3 * i) + 2)\n",
    "    plt.axis(False)\n",
    "    plt.grid(False)\n",
    "    plt.title(f\"Prediction - {i + 1}\")\n",
    "    plt.imshow(pred[i])\n",
    "\n",
    "    plt.subplot(6, 6, (3 * i) + 3)\n",
    "    plt.axis(False)\n",
    "    plt.grid(False)\n",
    "    plt.title(f\"Normalized Error - {i + 1}\")\n",
    "    plt.imshow(norm_error[i])\n",
    "\n",
    "plt.savefig(os.path.join(PLOTS_DIR, 'predictions.jpg'), dpi = PLOTS_DPI, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Model Saving"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(os.path.join('models', MODEL_NAME))"
   ]
  }
 ]
}